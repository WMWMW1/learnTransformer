{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                      download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=64,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                        download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=64,\n",
    "                                          shuffle=False, num_workers=2)\n",
    "# 这是一个nmist数据集\n",
    "#\n",
    "#trainset是训练集，testset是测试集\n",
    "print(len(trainset))\n",
    "print(len(testset))\n",
    "#trainset和testset都是一个数据集，里面有60000个样本,被分成了两个数据集，一个是训练集，一个是测试集，确保训练集和测试集是独立的\n",
    "# print(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 784])\n",
      "torch.Size([64, 20])\n",
      "torch.Size([64, 20])\n",
      "tensor([[0.4154, 0.4601, 0.3943,  ..., 0.5111, 0.3938, 0.5057],\n",
      "        [0.4585, 0.4871, 0.5030,  ..., 0.4416, 0.5652, 0.5018],\n",
      "        [0.4122, 0.5386, 0.4323,  ..., 0.4447, 0.4600, 0.4008],\n",
      "        ...,\n",
      "        [0.4137, 0.5161, 0.4431,  ..., 0.4732, 0.4933, 0.4406],\n",
      "        [0.4346, 0.4951, 0.4104,  ..., 0.4737, 0.4944, 0.4698],\n",
      "        [0.5236, 0.5448, 0.3843,  ..., 0.5443, 0.4576, 0.4762]],\n",
      "       grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class CVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CVAE, self).__init__()\n",
    "        self.label_emb = nn.Embedding(10, 10)  # Embedding for the 10 classes\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(794, 400),  # 784 pixels + 10 label embedding = 794\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(400, 20)  # Latent space mean\n",
    "        self.fc_var = nn.Linear(400, 20)  # Latent space variance\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(30, 400),  # 20 latent dims + 10 label embedding\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400, 784),\n",
    "            nn.Sigmoid(),  # Output between 0 and 1\n",
    "        )\n",
    "\n",
    "    def encode(self, x, labels):\n",
    "        combined = torch.cat((x, self.label_emb(labels)), 1)\n",
    "        h1 = self.encoder(combined)\n",
    "        return self.fc_mu(h1), self.fc_var(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z, labels):\n",
    "        combined = torch.cat((z, self.label_emb(labels)), 1)\n",
    "        return self.decoder(combined)\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        mu, logvar = self.encode(x.view(-1, 784), labels)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z, labels), mu, logvar\n",
    "# test the model\n",
    "model = CVAE()\n",
    "x = torch.randn(64, 784)\n",
    "labels = torch.randint(0, 10, (64,))\n",
    "output = model(x, labels)\n",
    "print(output[0].shape)\n",
    "print(output[1].shape)\n",
    "print(output[2].shape)\n",
    "\n",
    "\n",
    "print(output[0])\n",
    "import torch.nn.functional as F\n",
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    recon_x = torch.clamp(recon_x, 0, 1)  # Ensure the reconstructions are in the valid range for BCE\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1711403463728/work/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [51,0,0], thread: [64,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1711403463728/work/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [51,0,0], thread: [65,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1711403463728/work/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [51,0,0], thread: [66,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1711403463728/work/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [51,0,0], thread: [67,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1711403463728/work/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [51,0,0], thread: [69,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1711403463728/work/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [51,0,0], thread: [70,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1711403463728/work/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [51,0,0], thread: [71,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1711403463728/work/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [51,0,0], thread: [72,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1711403463728/work/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [51,0,0], thread: [73,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1711403463728/work/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [51,0,0], thread: [74,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1711403463728/work/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [51,0,0], thread: [77,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1711403463728/work/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [51,0,0], thread: [78,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1711403463728/work/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [51,0,0], thread: [79,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1711403463728/work/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [51,0,0], thread: [80,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1711403463728/work/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [51,0,0], thread: [82,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1711403463728/work/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [51,0,0], thread: [85,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1711403463728/work/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [51,0,0], thread: [86,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1711403463728/work/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [51,0,0], thread: [87,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1711403463728/work/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [51,0,0], thread: [90,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1711403463728/work/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [51,0,0], thread: [92,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1711403463728/work/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [51,0,0], thread: [93,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1711403463728/work/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [51,0,0], thread: [94,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n",
      "/opt/conda/conda-bld/pytorch_1711403463728/work/aten/src/ATen/native/cuda/Loss.cu:95: operator(): block: [51,0,0], thread: [95,0,0] Assertion `target_val >= zero && target_val <= one` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n\u001b[1;32m     45\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m---> 46\u001b[0m \u001b[43mtrain_test_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 34\u001b[0m, in \u001b[0;36mtrain_test_loop\u001b[0;34m(model, train_loader, test_loader, optimizer, loss_function, num_epochs, device)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_test_loop\u001b[39m(model, train_loader, test_loader, optimizer, loss_function, num_epochs, device):\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 34\u001b[0m         train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m         test_loss \u001b[38;5;241m=\u001b[39m test(model, device, test_loader, loss_function)\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]: Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 12\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, loss_function)\u001b[0m\n\u001b[1;32m      9\u001b[0m recon_batch, mu, logvar \u001b[38;5;241m=\u001b[39m model(data, labels)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Compute and print loss\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecon_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogvar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[2], line 55\u001b[0m, in \u001b[0;36mloss_function\u001b[0;34m(recon_x, x, mu, logvar)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss_function\u001b[39m(recon_x, x, mu, logvar):\n\u001b[1;32m     54\u001b[0m     recon_x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(recon_x, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Ensure the reconstructions are in the valid range for BCE\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m     BCE \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecon_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m784\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     KLD \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m logvar \u001b[38;5;241m-\u001b[39m mu\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m-\u001b[39m logvar\u001b[38;5;241m.\u001b[39mexp())\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m BCE \u001b[38;5;241m+\u001b[39m KLD\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_HF/lib/python3.12/site-packages/torch/nn/functional.py:3127\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3124\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m   3125\u001b[0m     weight \u001b[38;5;241m=\u001b[39m weight\u001b[38;5;241m.\u001b[39mexpand(new_size)\n\u001b[0;32m-> 3127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_enum\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "def train(model, device, train_loader, optimizer, loss_function):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "        recon_batch, mu, logvar = model(data, labels)\n",
    "        \n",
    "        # Compute and print loss\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    average_loss = total_loss / len(train_loader.dataset)\n",
    "    return average_loss  # We return average loss to track over epochs\n",
    "def test(model, device, test_loader, loss_function):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            recon_batch, mu, logvar = model(data, labels)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    return test_loss\n",
    "\n",
    "def train_test_loop(model, train_loader, test_loader, optimizer, loss_function, num_epochs, device):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train(model, device, train_loader, optimizer, loss_function)\n",
    "        test_loss = test(model, device, test_loader, loss_function)\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}]: Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "model = CVAE().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 10\n",
    "train_test_loop(model, train_loader, test_loader, optimizer, loss_function, num_epochs, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, loss_function):\n",
    "    model.train()  \n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_function(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)  \n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        total += target.size(0)\n",
    "    \n",
    "    average_loss = total_loss / len(train_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    return average_loss, accuracy\n",
    "\n",
    "\n",
    "def test(model, device, test_loader, loss_function):\n",
    "    model.eval() # 将模型设置为评估模式\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = loss_function(output, target)\n",
    "            test_loss += loss.item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            total += target.size(0)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / total\n",
    "    return test_loss, accuracy\n",
    "\n",
    "#写一个循环来做训练和测试\n",
    "# train(mlp, trainloader, criterion, optimizer)\n",
    "# test(mlp, testloader, criterion）\n",
    "\n",
    "#现在我们把训练和测试的代码封装到一个函数里面\n",
    "\n",
    "def train_test_loop(model, train_loader, test_loader, optimizer, loss_function, num_epochs, device, save_path='model_checkpoint/', scheduler=None, checkpoint_filename=None, save_frequency=1):\n",
    "    start_epoch = 0\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        train_loss, train_accuracy = train(model, device, train_loader, optimizer, loss_function)\n",
    "        test_loss, test_accuracy = test(model, device, test_loader, loss_function)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/a/miniconda3/envs/llm_HF/lib/python3.12/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1711403463728/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "only batches of spatial targets supported (3D tensors) but got targets of size: : [64]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m)\n\u001b[1;32m      9\u001b[0m num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mtrain_test_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 52\u001b[0m, in \u001b[0;36mtrain_test_loop\u001b[0;34m(model, train_loader, test_loader, optimizer, loss_function, num_epochs, device, save_path, scheduler, checkpoint_filename, save_frequency)\u001b[0m\n\u001b[1;32m     50\u001b[0m start_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_epoch, num_epochs):\n\u001b[0;32m---> 52\u001b[0m     train_loss, train_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     test_loss, test_accuracy \u001b[38;5;241m=\u001b[39m test(model, device, test_loader, loss_function)\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m scheduler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, loss_function)\u001b[0m\n\u001b[1;32m      8\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m      9\u001b[0m output \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[0;32m---> 10\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_HF/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_HF/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_HF/lib/python3.12/site-packages/torch/nn/modules/loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_HF/lib/python3.12/site-packages/torch/nn/functional.py:3059\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3058\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: only batches of spatial targets supported (3D tensors) but got targets of size: : [64]"
     ]
    }
   ],
   "source": [
    "\n",
    "# 定义损失函数和优化器\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Autoencoder().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "num_epochs=4\n",
    "train_test_loop(model,train_loader, test_loader, \n",
    "                optimizer, \n",
    "                loss_function, \n",
    "                num_epochs,device,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([10, 20, 128])\n",
      "Attention weights shape: torch.Size([10, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, key_size, query_size, value_size):\n",
    "        \"\"\"\n",
    "        初始化注意力层\n",
    "        :param key_size: 键向量的维度\n",
    "        :param query_size: 查询向量的维度\n",
    "        :param value_size: 值向量的维度\n",
    "        \"\"\"\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.key_size = key_size\n",
    "        self.query_size = query_size\n",
    "        self.value_size = value_size\n",
    "\n",
    "        # 定义权重矩阵\n",
    "        self.key_layer = nn.Linear(query_size, key_size, bias=False)\n",
    "        self.query_layer = nn.Linear(query_size, key_size, bias=False)\n",
    "        self.value_layer = nn.Linear(value_size, value_size, bias=False)\n",
    "\n",
    "    def forward(self, keys, queries, values, mask=None):\n",
    "        \"\"\"\n",
    "        前向传播方法\n",
    "        :param keys: 键向量\n",
    "        :param queries: 查询向量\n",
    "        :param values: 值向量\n",
    "        :param mask: 可选的掩码向量，用于遮盖不需要关注的部分\n",
    "        \"\"\"\n",
    "        # 计算查询、键、值\n",
    "        queries = self.query_layer(queries)\n",
    "        keys = self.key_layer(keys)\n",
    "        values = self.value_layer(values)\n",
    "\n",
    "        # 计算注意力分数\n",
    "        scores = torch.matmul(queries, keys.transpose(-2, -1)) / self.key_size**0.5\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "            \n",
    "        # 应用softmax\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "        # 计算加权和\n",
    "        output = torch.matmul(attention_weights, values)\n",
    "        return output, attention_weights\n",
    "\n",
    "# 示例用法\n",
    "key_size = 64\n",
    "query_size = 64\n",
    "value_size = 128\n",
    "batch_size = 10\n",
    "seq_length = 20\n",
    "\n",
    "# 创建模型\n",
    "attention = AttentionLayer(key_size, query_size, value_size)\n",
    "\n",
    "# 模拟输入数据\n",
    "keys = torch.rand(batch_size, seq_length, key_size)\n",
    "queries = torch.rand(batch_size, seq_length, query_size)\n",
    "values = torch.rand(batch_size, seq_length, value_size)\n",
    "\n",
    "# 运行注意力层\n",
    "output, attention_weights = attention(keys, queries, values)\n",
    "\n",
    "print(\"Output shape:\", output.shape)\n",
    "print(\"Attention weights shape:\", attention_weights.shape)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(my_transformer, self).__init__()\n",
    "        \n",
    "    def forward(self, x):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 定义损失函数和优化器\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = my_transformer().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "num_epochs=4\n",
    "# train_test_loop(model,train_loader, test_loader, \n",
    "#                 optimizer, \n",
    "#                 loss_function, \n",
    "#                 num_epochs,device,\n",
    "#                 )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
